import numpy as np
import random
import math
import itertools
import cv2
import pandas as pd
import open3d as o3d

from natsort import natsorted
from PIL import Image
from pathlib import Path
from os import listdir
from os.path import join, isdir, basename
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from imutils import grab_contours
from math import cos, sin, radians

#Packages for Instance Analysis - must be installed
from mpl_toolkits.mplot3d import Axes3D
from scipy.interpolate import griddata
import matplotlib.pyplot as plt

# TODO: Sometimes multiple images are present for one ID
# Remove one of these from data
DATA_ROOT = Path("./Versuche/Bahn_1")
OUT_FOLDER = Path("./results")
WEIGHT_DATA = Path("Wiegung.ods")

# CALIBRATION
MM_PER_PX = 1.0512
CAMERA_CALIBRATION = np.array(
    [
        [1.0, 0.0, 0.0, -4.9546268797383198e02],
        [0.0, 1.0, 0.0, -3.7512762883019587e02],
        [0.0, 0.0, 0.0, 8.6172850867968327e02],
        [0.0, 0.0, 6.7185019790088871e00, 0.0],
    ],
    dtype=np.float32,
)

# WHAT TO RUN?
PROCESS_IMAGES_WITH_SAM = False
PROCESS_MASKS_FOR_MWD = True
PROCESS_PCLS = False

# CROP PARAMETERS - DONT CHANGE!
LEFT = 328
TOP = 148
SIZE = 512

# MASK FILTER SIZE FOR OWN POSTPROCESSING
MAX_MASK_SIZE = 5000
MIN_MASK_SIZE_OVERLAPPING = 100  # should be higher than MIN_MASK_SIZE_NOT_OVERLAPPING
MIN_MASK_SIZE_NOT_OVERLAPPING = 15

# VISUALIZATION
# alpha of alpha mask
MASK_ALPHA = 0.6

# SAM AUTOMATIC MASK GENERATOR PARAMETERS
# grid resolution/ sampling
POINTS_PER_SIDE = 64
CROP_N_LAYERS = 2
CROP_N_POINTS_DOWNSCALE_FACTOR = 2

# GPU Batch-size - increase for speed - decrease for memory
POINTS_PER_BATCH = 64

# threshold to keep whole mask if predicted mask quality is higher than tresh # 0.88 default
PRED_IOU_THRESH = 0.86

# threshold when to apply non-max suppression on masks (Crop then between crops (see n-layers))
# higher
BOX_NMS_THRESH = 0.7
CROP_NMS_THRESH = 0.7
# overlap of the cropped parts
CROP_OVERLAP_RATIO = 512 / 1500

# STABILITY_SCORE_THRESH
# higher --> less accepted masks since IOU between high and low logit thresh must be higher
# lower --> more accepted masks since IOU between high and low logit thresh must not be that high
STABILITY_SCORE_THRESH = 0.92
# STABILITY_SCORE_OFFSET
# higher --> maybe worse masks are compared with very good masks for stability score (IOU)
# lower --> maybe better bad masks are compared with worse good masks for stability score (IOU)
# better keep this at 1 since it directly attacks at logit output
STABILITY_SCORE_OFFSET = 1

# opencv prosprocessing to remove splinters by automatic mask generator
MIN_MASK_REGION_AREA = 200


def get_img_file_names(dataroot: Path) -> list[list]:
    samples = [
        join(dataroot, directory)
        for directory in listdir(dataroot)
        if isdir(join(dataroot, directory))
    ]
    samples = natsorted(samples)
    img_files = []
    disp_files = []
    pcl_names = []
    ids = []
    for sample in samples:
        img_files.extend(
            [join(sample, file) for file in listdir(sample) if "left" in file]
        )
        disp_files.extend(
            [join(sample, file) for file in listdir(sample) if "disp_raw" in file]
        )
        pcl_names.extend(
            [join(sample, file) for file in listdir(sample) if ".ply" in file]
        )
        ids.append(basename(sample))
    return img_files, ids, pcl_names, disp_files


def create_colors(r_range: tuple, g_range: tuple, b_range: tuple) -> list[list]:
    r = [i for i in range(*r_range)]
    g = [i for i in range(*g_range)]
    b = [i for i in range(*b_range)]
    colors = list(map(list, itertools.product(r, g, b)))
    random.shuffle(colors)
    return colors


def process_masks(
    sorted_masks: list[dict],
    bin_mask: np.ndarray,
    res_mask: np.ndarray,
    colors: list[list],
    mask_alpha: float,
):
    count = 0
    # loop over all masks generated by sam sorted by size
    # start with small masks and end with large
    for mask in sorted_masks:
        # remove large masks
        if mask["area"] > MAX_MASK_SIZE:
            continue

        # remove pixels that are already occupied
        free_mask = mask["segmentation"] & bin_mask
        free_pixels = free_mask.sum()

        # abort and continue with next mask if too less
        # pixels are left after removing occupied pixels
        # threshold should be higher (MIN_MASK_SIZE_OVERLAPPING)
        if free_pixels < MIN_MASK_SIZE_NOT_OVERLAPPING:
            continue

        # if some pixels were removed from sam-mask, we need to
        # check if the left-over pixels are connected and only keep largest
        # block of connected pixels
        free_ratio = free_pixels / mask["area"]
        if free_ratio < 1.0:
            cnts = cv2.findContours(
                np.array(free_mask, dtype="uint8"),
                cv2.RETR_EXTERNAL,
                cv2.CHAIN_APPROX_NONE,
            )
            # find and draw largest connected pixel cluster
            cnts = grab_contours(cnts)
            sorted_cnts = sorted(cnts, key=lambda x: cv2.contourArea(x))

            large_cnt = sorted_cnts[-1]
            cnt_mask = np.zeros((free_mask.shape[0], free_mask.shape[1]), dtype="uint8")
            cnt_mask = cv2.drawContours(
                cnt_mask, [large_cnt], -1, color=1, thickness=-1
            )
            large_cnt_pixels = cnt_mask.sum()

            # if this cluster is too small, we skip the mask
            # threshold here is higher since we dont detect small clods at this point
            if large_cnt_pixels < MIN_MASK_SIZE_OVERLAPPING:
                print("largest part of mask too small with size:", large_cnt_pixels)
                continue

            free_mask = np.array(cnt_mask, dtype=np.bool_)

        # get a random color for the mask and draw it with alpha on the resulting mask
        # remove the used pixels from the binary masks. They are now occupied
        color = colors.pop()
        res_mask[free_mask] = [*color, int(mask_alpha * 255)]
        bin_mask[free_mask] = False

        count += 1
    print(f"created {count} clods")


def process_img(
    img_file: Path,
    id: int,
    sam_mask_generator: SamAutomaticMaskGenerator,
    mask_alpha: float = 0.8,
):

    print(f"-----------------------------\nID: {id}")
    # open and cut image
    pil_img = Image.open(img_file)
    img = np.array(pil_img)
    test_img = img.copy()
    cv2.rectangle(
        test_img, (LEFT, TOP), (LEFT + SIZE, TOP + SIZE), color=(0, 0, 0), thickness=1
    )
    img = img[TOP : TOP + SIZE, LEFT : LEFT + SIZE]
    Image.fromarray(test_img).save(join(OUT_FOLDER, f"cut_test_{id}.png"))
    Image.fromarray(img).save(join(OUT_FOLDER, f"cut_{id}.png"))

    # generate random colors
    colors = create_colors(
        r_range=(50, 201, 10), g_range=(50, 201, 10), b_range=(50, 201, 10)
    )

    res_mask = np.ones((img.shape[0], img.shape[1], 4), dtype="uint8")
    bin_mask = np.ones((img.shape[0], img.shape[1]), dtype=np.bool_)

    # auto-promt sam
    masks = sam_mask_generator.generate(img)
    print(f"found {len(masks)} masks")
    sorted_masks = sorted(masks, key=lambda x: x["area"])
    # sorted_masks = sorted(masks, key=lambda x: x["area"], reverse=True)

    # post processing
    process_masks(
        sorted_masks=sorted_masks,
        bin_mask=bin_mask,
        res_mask=res_mask,
        colors=colors,
        mask_alpha=mask_alpha,
    )

    # save the result
    mask = res_mask[:, :, :3]
    mask = Image.fromarray(mask)
    mask.save(join(OUT_FOLDER, f"final_mask_{id}.png"))

    # save some visualizations
    black_img = img.copy()
    black_img[bin_mask] = (0, 0, 0)
    black_img = Image.fromarray(black_img)
    black_img.save(join(OUT_FOLDER, f"black_{id}.png"))

    black_img_inv = img.copy()
    black_img_inv[np.invert(bin_mask)] = (0, 0, 0)
    black_img_inv = Image.fromarray(black_img_inv)
    black_img_inv.save(join(OUT_FOLDER, f"black_inv_{id}.png"))

    alpha_img = np.zeros((img.shape[0], img.shape[1], 4), dtype="uint8")
    alpha_img[:, :, :3] = img
    alpha_img[:, :, 3] = 255

    alpha_img = Image.fromarray(alpha_img, mode="RGBA")
    res_mask = Image.fromarray(res_mask, mode="RGBA")

    out = Image.alpha_composite(alpha_img, res_mask)
    out.save(join(OUT_FOLDER, f"alpha_mask_{id}.png"))


def ellipsis_analysis(
    mask: np.ndarray, colors: np.ndarray, counts: np.ndarray
) -> tuple[list]:
    # use smallest diameter of fitted ellipse
    minor_diameters = []
    major_diameters = []
    areas = []
    ellipses = []

    ignored_colors = []
    ellipse_colors = []


    for color, count in zip(colors, counts):
        if list(color) == [1, 1, 1]:
            continue
        clod_mask = np.zeros((mask.shape[0], mask.shape[1], 1), dtype="uint8")
        clod_mask[np.where(np.all(mask == color, axis=-1))] = 255
        rows, cols = np.where(np.all(mask == color, axis=-1))
        (xc, yc), (d1, d2), angle = cv2.fitEllipse(np.dstack([cols, rows]))

        x_center = int(xc)
        y_center = int(yc)
        if x_center >= mask.shape[0] or y_center >= mask.shape[1]:
            # TODO: solve this edge case
            print(
                f"Center of ellipse not in image shape at x:{x_center}, y:{y_center} - ignoring clod..."
            )
            ignored_colors.append(color)
            continue

        if clod_mask[y_center, x_center] != 255:
            print(
                f"Center of ellipse not in clod at x:{x_center}, y:{y_center} - ignoring clod..."
            )
            ignored_colors.append(color)
            continue

        clod_mask = np.zeros((mask.shape[0], mask.shape[1], 1), dtype="uint8")

        # draw major axis line in red
        rmajor = max(d1, d2) / 2
        if angle > 90:
            angle = angle - 90
        else:
            angle = angle + 90
        x1_maj = xc + cos(radians(angle)) * rmajor
        y1_maj = yc + sin(radians(angle)) * rmajor
        x2_maj = xc + cos(radians(angle + 180)) * rmajor
        y2_maj = yc + sin(radians(angle + 180)) * rmajor

        # draw minor axis line in blue
        rminor = min(d1, d2) / 2
        if angle > 90:
            angle = angle - 90
        else:
            angle = angle + 90

        x1_min = xc + cos(radians(angle)) * rminor
        y1_min = yc + sin(radians(angle)) * rminor
        x2_min = xc + cos(radians(angle + 180)) * rminor
        y2_min = yc + sin(radians(angle + 180)) * rminor

        dist_minor_axis = (
            np.linalg.norm(np.array([x1_min, y1_min]) - [x2_min, y2_min]) * MM_PER_PX
        )
        dist_major_axis = (
            np.linalg.norm(np.array([x1_maj, y1_maj]) - [x2_maj, y2_maj]) * MM_PER_PX
        )

        minor_diameters.append(dist_minor_axis)
        areas.append(count * MM_PER_PX**2)
        major_diameters.append(dist_major_axis)
        ellipses.append({"center": (xc, yc), "axes": (d1, d2), "angle": angle})
        ellipse_colors.append(color.tolist())


    return (minor_diameters, major_diameters, areas, ellipses, ignored_colors, ellipse_colors)


def create_ellipsoids(minor_diameters, major_diameters, ellipses, depth_map):
    # Calculate additional histograms for middle_diameter and volume_ellipsoid
    middle_diameters = []
    large_diameters = []
    volumes = []
    for minor, major, ellipse in zip(minor_diameters, major_diameters, ellipses):
        min_depth, max_depth, diff_depth = extract_depth_range_within_ellipse(
            depth_map, ellipse
        )

        # Find the middle value of the three measurements
        middle_diameter = sorted([minor, major, diff_depth / MM_PER_PX])[1]
        large_diameter = sorted([minor, major, diff_depth / MM_PER_PX])[-1]
        middle_diameters.append(middle_diameter)
        large_diameters.append(large_diameter)

        # Calculate the volume of the ellipsoid
        a = minor / 2  # converting diameters to radien
        b = major / 2
        c = diff_depth / 2
        volume = (4 / 3) * math.pi * a * b * c
        volumes.append(volume)

    return middle_diameters, large_diameters, volumes, min_depth, max_depth


def save_ellipsoid_to_csv(
    id: int,
    minor_diameters: list,
    major_diameters: list,
    areas: list,
    ellipses: list[dict],
    depth_map: np.ndarray,
    output_folder: Path,
):
    depths_min = []
    depths_max = []
    depths_diff = []
    middle_diameters = []
    volumes = []

    for minor, major, ellipse in zip(minor_diameters, major_diameters, ellipses):
        min_depth, max_depth, diff_depth = extract_depth_range_within_ellipse(
            depth_map, ellipse
        )
        depths_min.append(min_depth)
        depths_max.append(max_depth)
        depths_diff.append(diff_depth)

        # Find the middle value of the three measurements
        middle_diameter = sorted([minor, major, diff_depth])[1]
        middle_diameters.append(middle_diameter)

        # Calculate the volume of the ellipsoid
        a = minor / 2  # converting diameters to radii
        b = major / 2
        c = diff_depth / 2
        volume = (4 / 3) * math.pi * a * b * c
        volumes.append(volume)

    print("min_depth", min(depths_min))
    print("max_depth", max(depths_max))
    df = pd.DataFrame(
        {
            "minor_diameters": minor_diameters,
            "major_diameters": major_diameters,
            "areas": areas,
            "ellipses": [str(e) for e in ellipses],
            "depth_min": depths_min,
            "depth_max": depths_max,
            "depth_diff": depths_diff,
            "middle_diameter": middle_diameters,
            "volume_ellipsoid": volumes,
        }
    )

    output_file = output_folder / f"ellipsoid_{id}.csv"
    df.to_csv(output_file, index=False)
    print(f"Ellipses data saved to {output_file}")


def visual_ellipis_analysis(
    depth_map: np.ndarray,
    ellipses: dict,
    data_idx: int,
    ignored_colors: list,
    mask: np.ndarray,
):
    # save a mask with all ellipsis drawn
    ellipsis_mask = np.zeros(depth_map.shape[:2], dtype=np.uint8)
    for ellipse in ellipses:
        center = (int(ellipse["center"][0]), int(ellipse["center"][1]))
        axes = (int(ellipse["axes"][0] / 2), int(ellipse["axes"][1] / 2))
        angle = ellipse["angle"]
        cv2.ellipse(ellipsis_mask, center, axes, angle, 0, 360, 255, 1)

    Image.fromarray(ellipsis_mask).save(join(OUT_FOLDER, f"z_test_clod{data_idx}.png"))

    filtered_mask = mask.copy()
    ignored_clods_mask = np.zeros(depth_map.shape[:2], dtype=np.uint8)

    palette = np.array(ignored_colors).transpose()
    rep_color = np.array([1, 1, 1], dtype="uint8")
    remove_mask = (filtered_mask[:, :, :, None] == palette).all(2).any(-1)
    ignored_clods_mask = np.where(
        remove_mask[:, :, None], filtered_mask, rep_color[None, None, :]
    )
    for color in ignored_colors:
        filtered_mask[np.where(np.all(mask == color, axis=-1))] = [1, 1, 1]
    Image.fromarray(filtered_mask).save(
        join(OUT_FOLDER, f"z_removed_ignored_clods{data_idx}.png")
    )
    Image.fromarray(ignored_clods_mask).save(
        join(OUT_FOLDER, f"z_only_ignored_clods{data_idx}.png")
    )


def create_mwd(
    bins: list,
    hist_array: np.ndarray,
    total: float,
    diameters_for_mwd: list,
    weights: list,
    background_count: int,
    mwd_name: str,
    result_dict: dict,
):
    bin_count, _ = np.histogram(hist_array, bins=bins, weights=weights)
    bin_values_mwd = [dia * area for (dia, area) in zip(diameters_for_mwd, bin_count)]

    mwd_sum = sum(bin_values_mwd)
    mwd = mwd_sum / total

    mwd_background = (mwd_sum + background_count * MM_PER_PX**2 * 2.5) / (
        512**2 * MM_PER_PX**2
    )

    result_dict[mwd_name] = mwd
    result_dict[
        f"{mwd_name}_with_background_(in smallest_two_sieves)_as_area(not_volume)"
    ] = mwd_background
    for idx, bin_val in enumerate(bin_values_mwd):
        result_dict[f"{mwd_name}_bin_val_{idx}"] = bin_val
    return result_dict


def process_clods(id: int, depth_map: np.ndarray) -> dict:
    # print(np.unique(depth_map[:, :, 2], return_counts=True))
    print(f"-----------------------------\nID: {id}")
    result_dict = {}
    result_dict["id"] = id
    # open mask
    pil_mask = Image.open(Path(join(OUT_FOLDER, f"./final_mask_{id}.png")))
    mask = np.array(pil_mask)
    colors, counts = np.unique(
        mask.reshape(-1, mask.shape[-1]), axis=0, return_counts=True
    )

    background_idx = np.where(colors.all() == 1)
    background_px_count = counts[background_idx][0]
    result_dict["background_px_count"] = background_px_count
    result_dict["background_area_weighted_with_smallest_2_sieves(2.5)"] = (
        background_px_count * MM_PER_PX**2 * 2.5
    )
    colors = colors.tolist()
    counts = counts.tolist()
    colors.pop(background_idx[0].item())
    counts.pop(background_idx[0].item())
    colors = np.array(colors)
    counts = np.array(counts)

    diameters_of_sieves = [1.25, 2.5, 7.5, 15.0, 30.0, 60.0]
    diameters_for_bins = [0, 1.25, 2.5, 7.5, 15.0, 30.0, 60.0]

    # Pixelfläche und Fläche des Siebes
    bins = [d**2 / 4 for d in diameters_for_bins]
    result_dict = create_mwd(
        bins=bins,
        hist_array=counts * MM_PER_PX**2,
        total=counts.sum() * MM_PER_PX**2,
        diameters_for_mwd=diameters_of_sieves,
        weights=counts * MM_PER_PX**2,
        background_count=background_px_count,
        mwd_name="mwd_px_area_sieve_area",
        result_dict=result_dict,
    )
    
    # Ellipse und kleinere Achse + Siebdurchmesser
    minor_diameters, major_diameters, areas, ellipses, ignored_colors, ellipse_colors = (
        ellipsis_analysis(mask=mask, colors=colors, counts=counts)
    )

    # For visual ellipsis analysis:
    # visual_ellipis_analysis(
    #     depth_map=depth_map,
    #     ellipses=ellipses,
    #     data_idx=id,
    #     ignored_colors=ignored_colors,
    #     mask=mask,
    # )

    remove_idcs = []
    for ignored_color in ignored_colors:
        remove_idcs.append(np.where(np.all(colors == ignored_color, axis=-1)))

    colors = colors.tolist()
    counts = counts.tolist()
    remove_idcs = [idx[0].item() for idx in remove_idcs]
    for remove_idx in sorted(remove_idcs, reverse=True):
        colors.pop(remove_idx)
        counts.pop(remove_idx)
    colors = np.array(colors)
    counts = np.array(counts)

    bins = [0, 2.5, 5.0, 10.0, 20.0, 40.0, 80.0]
    result_dict = create_mwd(
        bins=bins,
        hist_array=minor_diameters,
        total=sum([area * MM_PER_PX**2 for area in areas]),
        diameters_for_mwd=diameters_of_sieves,
        weights=[area * MM_PER_PX**2 for area in areas],
        background_count=background_px_count,
        mwd_name="mwd_ellpsis_area_sieve_diameter",
        result_dict=result_dict,
    )

    bins = [0, 2.5, 5.0, 10.0, 20.0, 40.0, 80.0]
    result_dict = create_mwd(
        bins=bins,
        hist_array=minor_diameters,
        total=counts.sum() * MM_PER_PX**2,
        diameters_for_mwd=diameters_of_sieves,
        weights=counts * MM_PER_PX**2,
        background_count=background_px_count,
        mwd_name="mwd_ellpsis_with_mask_area_sieve_diameter",
        result_dict=result_dict,
    )

    # print("mwd_px_area_sieve_area", mwd_px_area_sieve_area)
    # print("mwd_ellpsis_area_sieve_diameter", mwd_ellpsis_area_sieve_diameter)

    # 3D Ellipsoid
    middle_diameters, large_diameters, volumes, min_depth, max_depth = (
        create_ellipsoids(minor_diameters, major_diameters, ellipses, depth_map)
    )
    bins = [0, 2.5, 5.0, 10.0, 20.0, 40.0, 80.0]
    result_dict = create_mwd(
        bins=bins,
        hist_array=middle_diameters,
        total=sum([area * MM_PER_PX**2 for area in areas]),
        diameters_for_mwd=diameters_of_sieves,
        weights=[area * MM_PER_PX**2 for area in areas],
        background_count=background_px_count,
        mwd_name="mwd_ellpsoid_area_sieve_diameter",
        result_dict=result_dict,
    )

    result_dict = create_mwd(
        bins=bins,
        hist_array=middle_diameters,
        total=sum([vol * MM_PER_PX**3 for vol in volumes]),
        diameters_for_mwd=diameters_of_sieves,
        weights=[vol * MM_PER_PX**3 for vol in volumes],
        background_count=background_px_count,
        mwd_name="mwd_ellpsoid_volume_sieve_diameter",
        result_dict=result_dict,
    )

    approx_vols = [
        4 / 3 * math.pi * mid_dia**2 / 4 * large_dia / 2 * MM_PER_PX**3
        for (mid_dia, large_dia) in zip(middle_diameters, large_diameters)
    ]
    result_dict = create_mwd(
        bins=bins,
        hist_array=middle_diameters,
        total=sum(approx_vols),
        diameters_for_mwd=diameters_of_sieves,
        weights=approx_vols,
        background_count=background_px_count,
        mwd_name="mwd_ellpsoid_approx_volume_sieve_diameter",
        result_dict=result_dict,
    )

    # Instance Analysis ausführen
    instance_data = instance_analysis(mask, colors, counts)

    # print("mwd_ellpsoid_area_sieve_diameter", mwd_ellpsoid_area_sieve_diameter)
    # print(
    #     "mwd_ellpsoid_area_sieve_diameter_background",
    #     mwd_ellpsoid_area_sieve_diameter_background,
    # )
    # print("mwd_ellpsoid_volume_sieve_diameter", mwd_ellpsoid_volume_sieve_diameter)
    # print(
    #     "mwd_ellpsoid_volume_sieve_diameter_background",
    #     mwd_ellpsoid_volume_sieve_diameter_background,
    # )
    # print(
    #     "mwd_ellpsoid_approx_volume_sieve_diameter",
    #     mwd_ellpsoid_approx_volume_sieve_diameter,
    # )
    # print(
    #     "mwd_ellpsoid_approx_volume_sieve_diameter_background",
    #     mwd_ellpsoid_approx_volume_sieve_diameter_background,
    # )

    # Save ellipses data to CSV with depth information
    # save_ellipsoid_to_csv(
    #     id, minor_diameters, major_diameters, areas, ellipses, depth_map, OUT_FOLDER
    # )

    # Berechnung von Ellipsendaten und Instanzdaten
    # ellipses_data = ellipsis_analysis(mask, colors, counts)
    # instance_data = instance_analysis(mask, colors, counts)

    # # Speichern der Instanzdaten zusammen mit den Ellipsendaten
    # instance_ellipse_to_csv(id, ellipses_data, instance_data, OUT_FOLDER)

    # Instanzanalyse durchführen
    pil_mask = Image.open(Path(join(OUT_FOLDER, f"./final_mask_{id}.png")))
    mask = np.array(pil_mask)
    colors, counts = np.unique(
        mask.reshape(-1, mask.shape[-1]), axis=0, return_counts=True
    )
    instance_colors, instance_pixels, instance_coordinates = instance_analysis(mask, colors, counts)


    instances_3d = instance_analysis_3d(instance_coordinates, depth_map, mask.shape)

    # 3D-Modelle erstellen
    coords_3d_instances = create_3d_model_from_depths(instances_3d)

    # Visualisiere die erste Instanz
    visualize_3d_instance(coords_3d_instances[0])
    return result_dict


def extract_depth_range_within_ellipse(depth_map, ellipse):
    mask = np.zeros(depth_map.shape[:2], dtype=np.uint8)
    center = (int(ellipse["center"][0]), int(ellipse["center"][1]))
    axes = (int(ellipse["axes"][0] / 2), int(ellipse["axes"][1] / 2))
    angle = ellipse["angle"]
    cv2.ellipse(mask, center, axes, angle, 0, 360, 255, -1)
    if np.unique(mask).shape[0] == 1:
        print("Could not fin ellipse on mask")
    masked_depth = np.where(mask == 255, depth_map[:, :, 2], np.nan)
    masked_depth[masked_depth < 0.55] = np.nan
    min_depth = np.nanmin(masked_depth)
    max_depth = np.nanmax(masked_depth)
    return min_depth, max_depth, (max_depth - min_depth) * 1000


def load_point_cloud(fname: Path):
    pcl = o3d.io.read_point_cloud(fname)
    # TODO: Tune this outlier detection and check it by visualization
    pcl_new, out_list = pcl.remove_radius_outlier(20, 0.01, print_progress=True)
    np_pcl = np.asarray(pcl_new.points)

    width = np_pcl[:, 0].max() - np_pcl[:, 0].min()
    height = np_pcl[:, 1].max() - np_pcl[:, 1].min()

    # calculate real world size to estimate mm/px --> ~ 1.04
    # use height for it --> width has blind border on the left side
    print("-------------\nwidth")
    print(width)
    print(width / 1024 * 1000)
    print("height")
    print(height)
    print(height / 768 * 1000)
    # TODO: Continue - maybe better read disp map and convert to depth


def create_overall_result_csv(mwd_results_fname: Path, weight_data_fname: Path):
    mwd_results_df = pd.read_csv(mwd_results_fname, sep=";", index_col="id")
    weight_df = pd.read_excel(weight_data_fname, engine="odf", index_col="ID")
    # filter the rows for the IDs - drop sub_ids
    only_full_id_df = weight_df.dropna(subset=["MWD"])
    joined_df = pd.concat([only_full_id_df, mwd_results_df], axis=1)
    joined_df.to_csv("joined_results.csv", sep=";")


def process_disp(disp_fname: Path):
    disp = cv2.imread(disp_fname, cv2.IMREAD_UNCHANGED)
    depth = (
        cv2.reprojectImageTo3D(
            disparity=np.array(disp / 16, dtype=np.float32), Q=CAMERA_CALIBRATION
        )
        * 16
    )
    depth_roi = depth[TOP : TOP + SIZE, LEFT : LEFT + SIZE]
    dim0 = depth_roi[:, :, 0].max() - depth_roi[:, :, 0].min()
    dim1 = depth_roi[:, :, 1].max() - depth_roi[:, :, 1].min()

    # TODO: USE depth to estimate 3D Objects from mask

    return depth_roi, (dim0 / 512 * 1000 + dim1 / 512 * 1000) / 2


def instance_analysis(mask: np.ndarray, colors: np.ndarray, counts: np.ndarray) -> tuple:
    instance_colors = []
    instance_pixels = []
    instance_coordinates = []
    
    for color, count in zip(colors, counts):
        # Überprüfen, ob die Farbe nicht dem Hintergrund entspricht
        if not np.array_equal(color, [1, 1, 1]):  # Annahme, dass [1, 1, 1] der Hintergrund ist
            # Erstelle eine Maske für die aktuelle Farbe
            mask_by_color = np.all(mask == color.reshape(1, 1, 3), axis=2)
            
            # Zähle die Pixel
            coordinates = np.argwhere(mask_by_color)
            area = coordinates.shape[0]  # Da `coordinates` alle Pixelkoordinaten enthält, entspricht die Anzahl der Zeilen der Fläche
            
            # Invertieren Sie die Koordinaten in (x, y)-Reihenfolge
            coordinates = [(x, y) for y, x in coordinates]
            
            # Füge die Informationen zu den Listen hinzu
            instance_colors.append(color.tolist())  # Konvertieren des Arrays zu einer Liste für einfache Lesbarkeit
            instance_pixels.append(area)  # Anzahl der Pixel
            instance_coordinates.append(coordinates)  # Koordinaten in (x, y) Reihenfolge

    return instance_colors, instance_pixels, instance_coordinates


def instance_analysis_3d(instance_coordinates: list, depth_roi: np.ndarray, mask_shape: tuple) -> list:
    instances_3d = []

    for coordinates in instance_coordinates:
        # Ermitteln der Tiefeninformationen für jedes Pixel innerhalb der Instanz
        depths = [depth_roi[y, x, 2] for x, y in coordinates]
        
        # Finden der benachbarten Pixel außerhalb der Instanz
        neighboring_pixels = set()
        for x, y in coordinates:
            # Prüfen der Nachbarn, um zu bestimmen, ob es sich um einen Randpixel handelt
            neighbors = [(x-1, y), (x+1, y), (x, y-1), (x, y+1)]
            for nx, ny in neighbors:
                if 0 <= nx < mask_shape[1] and 0 <= ny < mask_shape[0]:
                    if (nx, ny) not in coordinates:
                        neighboring_pixels.add((nx, ny))
        
        # Ermitteln der Tiefeninformationen für die benachbarten Pixel
        neighbor_depths = [depth_roi[ny, nx, 2] for nx, ny in neighboring_pixels]
        
        # Bestimme die größte Tiefe als Bodentiefe (ground_depth)
        all_depths = depths + neighbor_depths
        ground_depth = max(all_depths) if all_depths else 0
        
        # Berechnung der relativen Tiefen
        relative_depths = [(ground_depth - depth) * 1000 / MM_PER_PX for depth in depths]

        # Füge die 3D-Informationen zur Liste hinzu
        instance_3d = [(x, y, depth) for (x, y), depth in zip(coordinates, relative_depths)]
        instances_3d.append(instance_3d)
    print("instances_3d:", instances_3d)  # Ausgabe der instances_3d Liste zur Überprüfung
    return instances_3d

def create_3d_model_from_depths(instances_3d: list) -> list:
    coords_3d_instances = []

    for instance in instances_3d:
        coordinates = instance

        instance_3d = []
        for (x, y, depth) in coordinates:
            num_voxels = round(depth)  # Anzahl der Tiefenpixel bestimmen

            # Erstellen der 3D-Koordinaten relativ zum Mittelpunkt
            half_voxels = num_voxels // 2

            # Erstellen der 3D-Koordinaten relativ zum Mittelpunkt
            for z in range(-half_voxels, half_voxels + 1):
                instance_3d.append((x, y, z))

        coords_3d_instances.append(instance_3d)

    return coords_3d_instances

def visualize_3d_instance(instance_3d: list):
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')

    x_coords, y_coords, z_coords = zip(*instance_3d)

    # Erstellung eines booleschen Gitters, das die Voxel repräsentiert
    x_coords = np.array(x_coords)
    y_coords = np.array(y_coords)
    z_coords = np.array(z_coords)

    # Bestimme die Ausdehnung des Gitters
    x_extent = x_coords.max() - x_coords.min() + 1
    y_extent = y_coords.max() - y_coords.min() + 1
    z_extent = z_coords.max() - z_coords.min() + 1

    # Erstelle ein leeres Gitter
    grid = np.zeros((x_extent, y_extent, z_extent), dtype=bool)

    # Fülle das Gitter basierend auf den Koordinaten
    for x, y, z in zip(x_coords, y_coords, z_coords):
        grid[x - x_coords.min(), y - y_coords.min(), z - z_coords.min()] = True

    # Plotten der Voxel
    ax.voxels(grid, facecolors='blue', edgecolor='k')

    ax.set_xlabel('X Label')
    ax.set_ylabel('Y Label')
    ax.set_zlabel('Z Label')

    plt.show()



def instance_ellipse_to_csv(id: int, ellipses_data: tuple, instance_data: tuple, output_folder: Path):
    instance_colors, instance_pixels, instance_coordinates = instance_data
    _, _, ellipse_areas, ellipses, _, ellipse_colors = ellipses_data

    data = {
        "Color": [],
        "Instance Area": [],
        "Coordinates": [],
        "Ellipse Area": [],
        "Ellipse Center X": [],
        "Ellipse Center Y": [],
        "Ellipse Axis 1": [],
        "Ellipse Axis 2": [],
        "Ellipse Angle": []
    }

    for color, pixels, coords in zip(instance_colors, instance_pixels, instance_coordinates):
        for ell_color, ell_area, ellipse in zip(ellipse_colors, ellipse_areas, ellipses):
            if color == ell_color:  # Prüfen, ob die Farben übereinstimmen
                data["Color"].append(color)
                data["Instance Area"].append(pixels)
                data["Coordinates"].append("; ".join(map(str, coords)))
                data["Ellipse Area"].append(ell_area * MM_PER_PX**2)  # Korrektur: Flächenberechnung
                data["Ellipse Center X"].append(ellipse['center'][0])
                data["Ellipse Center Y"].append(ellipse['center'][1])
                data["Ellipse Axis 1"].append(ellipse['axes'][0])
                data["Ellipse Axis 2"].append(ellipse['axes'][1])
                data["Ellipse Angle"].append(ellipse['angle'])
                break  # Wenn die passende Ellipse gefunden ist, die innere Schleife beenden

    df = pd.DataFrame(data)
    output_file = output_folder / f"instances_ellipses_{id}.csv"
    df.to_csv(output_file, index=False)
    print(f"Instance and ellipse data saved to {output_file}")


def main():
    imgs, ids, pcl_names, disps = get_img_file_names(DATA_ROOT)

    OUT_FOLDER.mkdir(parents=True, exist_ok=True)

    if PROCESS_IMAGES_WITH_SAM:
        sam_checkpoint = "sam_vit_h_4b8939.pth"
        model_type = "vit_h"
        device = "cuda"

        sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
        sam.to(device=device)
        mask_generator = SamAutomaticMaskGenerator(
            sam,
            points_per_side=POINTS_PER_SIDE,
            points_per_batch=POINTS_PER_BATCH,
            pred_iou_thresh=PRED_IOU_THRESH,
            stability_score_thresh=STABILITY_SCORE_THRESH,
            stability_score_offset=STABILITY_SCORE_OFFSET,
            box_nms_thresh=BOX_NMS_THRESH,
            crop_n_layers=CROP_N_LAYERS,
            crop_nms_thresh=CROP_NMS_THRESH,
            crop_overlap_ratio=CROP_OVERLAP_RATIO,
            crop_n_points_downscale_factor=CROP_N_POINTS_DOWNSCALE_FACTOR,
            min_mask_region_area=MIN_MASK_REGION_AREA,
        )

    rows = []
    mm_per_px_list = []
    for id, img_file, pcl_file, disp in zip(ids, imgs, pcl_names, disps):
        # if not id == "15":
        #     continue
        if PROCESS_IMAGES_WITH_SAM:
            process_img(
                img_file=img_file,
                id=id,
                sam_mask_generator=mask_generator,
                mask_alpha=MASK_ALPHA,
            )
        if PROCESS_MASKS_FOR_MWD:
            depth_map, avg_dim = process_disp(disp_fname=disp)
            mm_per_px_list.append(avg_dim)
            rows.append(process_clods(id=id, depth_map=depth_map))

        if PROCESS_PCLS:
            load_point_cloud(pcl_file)

    print("Mean mm per px:", sum(mm_per_px_list) / len(mm_per_px_list))

    if PROCESS_MASKS_FOR_MWD:
        df = pd.DataFrame(rows)
        df.to_csv(join(OUT_FOLDER, "mwd_results.csv"), sep=";", index=False)

    create_overall_result_csv(
        mwd_results_fname=Path(join(OUT_FOLDER, "mwd_results.csv")),
        weight_data_fname=WEIGHT_DATA,
    )


if __name__ == "__main__":
    main()
